# üöÄ PySpark Solution: Products & Categories Mapping

> **–†–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –Ω–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PySpark**

## üìã –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏

–í PySpark –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞–º–∏ (`pyspark.sql.DataFrame`) –∑–∞–¥–∞–Ω—ã –ø—Ä–æ–¥—É–∫—Ç—ã, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –∏—Ö —Å–≤—è–∑–∏:
- –ö–∞–∂–¥–æ–º—É –ø—Ä–æ–¥—É–∫—Ç—É –º–æ–∂–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å **–Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–ª–∏ –Ω–∏ –æ–¥–Ω–æ–π**
- –ö–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –º–æ–∂–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å **–Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏–ª–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ**

**–¶–µ–ª—å:** –ù–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç–æ–¥ –Ω–∞ PySpark, –∫–æ—Ç–æ—Ä—ã–π –≤ –æ–¥–Ω–æ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ –≤–µ—Ä–Ω–µ—Ç:
- ‚úÖ –í—Å–µ –ø–∞—Ä—ã ¬´–ò–º—è –ø—Ä–æ–¥—É–∫—Ç–∞ ‚Äì –ò–º—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏¬ª
- ‚úÖ –ò–º–µ–Ω–∞ –≤—Å–µ—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤, —É –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π

## üóÇÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
pyspark-products-categories/
‚îú‚îÄ‚îÄ üìÑ get_product_category_pairs.py  # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ —Å —Ä–µ—à–µ–Ω–∏—è–º–∏
‚îú‚îÄ‚îÄ üìñ README.md                      # –≠—Ç–æ—Ç —Ñ–∞–π–ª
‚îú‚îÄ‚îÄ üîß requirements.txt               # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞
‚îî‚îÄ‚îÄ üìä example_usage.py               # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
```

## üõ†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

### –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- **Python**: 3.7+ 
- **Java**: 8 –∏–ª–∏ 11 (–¥–ª—è PySpark)
- **RAM**: –º–∏–Ω–∏–º—É–º 4GB (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 8GB+)

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ pip
pip install pyspark

# –ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏–∑ —Ñ–∞–π–ª–∞
pip install -r requirements.txt
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ Python
python --version

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Java
java -version

# –ü—Ä–æ–≤–µ—Ä–∫–∞ PySpark
python -c "import pyspark; print(pyspark.__version__)"
```

## üéØ –†–µ—à–µ–Ω–∏—è

–í –ø—Ä–æ–µ–∫—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã **2 –ø–æ–¥—Ö–æ–¥–∞** –∫ —Ä–µ—à–µ–Ω–∏—é –∑–∞–¥–∞—á–∏:

### 1Ô∏è‚É£ **–û—Å–Ω–æ–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ** (`get_product_category_pairs`)
- **–ü–æ–¥—Ö–æ–¥**: –î–≤–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö LEFT JOIN
- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: ‚ö° –í—ã—Å–æ–∫–∞—è (2-3 —Å—Ç–∞–¥–∏–∏ –≤ Spark)
- **–ß–∏—Ç–∞–µ–º–æ—Å—Ç—å**: üìñ –•–æ—Ä–æ—à–∞—è
- **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è**: Production –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### 2Ô∏è‚É£ **–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ** (`get_product_category_pairs_verbose`) 
- **–ü–æ–¥—Ö–æ–¥**: INNER JOIN + LEFT ANTI JOIN + UNION
- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: ‚ö° –°—Ä–µ–¥–Ω—è—è (3-4 —Å—Ç–∞–¥–∏–∏ –≤ Spark)
- **–ß–∏—Ç–∞–µ–º–æ—Å—Ç—å**: üìñ –û—Ç–ª–∏—á–Ω–∞—è
- **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è**: –û–±—É—á–µ–Ω–∏—è –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –í–∞—Ä–∏–∞–Ω—Ç 1: –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º (pyspark shell)

```bash
# –ó–∞–ø—É—Å–∫ PySpark shell
pyspark

# –í shell –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:
exec(open('get_product_category_pairs.py').read())
# –¢–µ–ø–µ—Ä—å —Ñ—É–Ω–∫—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
```

### –í–∞—Ä–∏–∞–Ω—Ç 2: –ó–∞–ø—É—Å–∫ –∫–∞–∫ —Å–∫—Ä–∏–ø—Ç

```bash
spark-submit get_product_category_pairs.py

# –ò–ª–∏ —á–µ—Ä–µ–∑ python (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞)
python get_product_category_pairs.py
```

### –í–∞—Ä–∏–∞–Ω—Ç 3: –ò–º–ø–æ—Ä—Ç –≤ –≤–∞—à –ø—Ä–æ–µ–∫—Ç

```python
from get_product_category_pairs import get_product_category_pairs
from pyspark.sql import SparkSession

# –°–æ–∑–¥–∞–Ω–∏–µ Spark —Å–µ—Å—Å–∏–∏
spark = SparkSession.builder.appName("MyApp").getOrCreate()

# –í–∞—à–∏ DataFrame'—ã
products_df = ...
categories_df = ...
links_df = ...

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
result = get_product_category_pairs(products_df, categories_df, links_df)
result.show()
```

## üìä –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

**–ü—Ä–æ–¥—É–∫—Ç—ã** (`products_df`):
```
+----------+-------------------+
|product_id|product_name       |
+----------+-------------------+
|1         |iPhone 15          |
|2         |MacBook Pro        |
|3         |Samsung Galaxy     |
|4         |–ó–∞–≥–∞–¥–æ—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç |  # ‚Üê –ü—Ä–æ–¥—É–∫—Ç –ë–ï–ó –∫–∞—Ç–µ–≥–æ—Ä–∏–π
+----------+-------------------+
```

**–ö–∞—Ç–µ–≥–æ—Ä–∏–∏** (`categories_df`):
```
+-----------+-------------+
|category_id|category_name|
+-----------+-------------+
|1          |–°–º–∞—Ä—Ç—Ñ–æ–Ω—ã    |
|2          |–ù–æ—É—Ç–±—É–∫–∏     |
|3          |–≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞  |
+-----------+-------------+
```

**–°–≤—è–∑–∏** (`product_category_links_df`):
```
+----------+-----------+
|product_id|category_id|
+----------+-----------+
|1         |1          |  # iPhone ‚Üí –°–º–∞—Ä—Ç—Ñ–æ–Ω—ã
|1         |3          |  # iPhone ‚Üí –≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞
|2         |2          |  # MacBook ‚Üí –ù–æ—É—Ç–±—É–∫–∏
|2         |3          |  # MacBook ‚Üí –≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞
|3         |1          |  # Samsung ‚Üí –°–º–∞—Ä—Ç—Ñ–æ–Ω—ã
|3         |3          |  # Samsung ‚Üí –≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞
# –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –ø—Ä–æ–¥—É–∫—Ç —Å ID=4 –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç!
+----------+-----------+
```

### –†–µ–∑—É–ª—å—Ç–∞—Ç

```python
result = get_product_category_pairs(products_df, categories_df, links_df)
result.show()
```

**–í—ã–≤–æ–¥:**
```
+-------------------+-------------+
|product_name       |category_name|
+-------------------+-------------+
|iPhone 15          |–°–º–∞—Ä—Ç—Ñ–æ–Ω—ã    |
|iPhone 15          |–≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞  |
|MacBook Pro        |–ù–æ—É—Ç–±—É–∫–∏     |
|MacBook Pro        |–≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞  |
|Samsung Galaxy     |–°–º–∞—Ä—Ç—Ñ–æ–Ω—ã    |
|Samsung Galaxy     |–≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞  |
|–ó–∞–≥–∞–¥–æ—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç |null         |  # ‚Üê –ü—Ä–æ–¥—É–∫—Ç –±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏!
+-------------------+-------------+
```

## üìù –ü–æ–ª–Ω—ã–π –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
from get_product_category_pairs import get_product_category_pairs

# –°–æ–∑–¥–∞–Ω–∏–µ Spark —Å–µ—Å—Å–∏–∏
spark = SparkSession.builder \
    .appName("ProductsCategoriesDemo") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–µ–º
products_schema = StructType([
    StructField("product_id", IntegerType(), True),
    StructField("product_name", StringType(), True)
])

categories_schema = StructType([
    StructField("category_id", IntegerType(), True),
    StructField("category_name", StringType(), True)
])

links_schema = StructType([
    StructField("product_id", IntegerType(), True),
    StructField("category_id", IntegerType(), True)
])

# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
products_data = [
    (1, "iPhone 15"),
    (2, "MacBook Pro"), 
    (3, "Samsung Galaxy"),
    (4, "–ó–∞–≥–∞–¥–æ—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç")
]

categories_data = [
    (1, "–°–º–∞—Ä—Ç—Ñ–æ–Ω—ã"),
    (2, "–ù–æ—É—Ç–±—É–∫–∏"),
    (3, "–≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞")
]

links_data = [
    (1, 1), (1, 3),  # iPhone –≤ –¥–≤—É—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
    (2, 2), (2, 3),  # MacBook –≤ –¥–≤—É—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö  
    (3, 1), (3, 3)   # Samsung –≤ –¥–≤—É—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
    # –ü—Ä–æ–¥—É–∫—Ç ID=4 –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
]

# –°–æ–∑–¥–∞–Ω–∏–µ DataFrame'–æ–≤
products_df = spark.createDataFrame(products_data, products_schema)
categories_df = spark.createDataFrame(categories_data, categories_schema)
links_df = spark.createDataFrame(links_data, links_schema)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è
result = get_product_category_pairs(products_df, categories_df, links_df)

# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
print("üéØ –†–ï–ó–£–õ–¨–¢–ê–¢:")
result.show(truncate=False)

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
print(f"üìä –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {result.count()}")
print(f"üîç –ü—Ä–æ–¥—É–∫—Ç–æ–≤ –±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {result.filter(result.category_name.isNull()).count()}")

# –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏
spark.stop()
```

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ä–µ–¥

### –õ–æ–∫–∞–ª—å–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞
```python
spark = SparkSession.builder \
    .appName("LocalDev") \
    .master("local[*]") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()
```

### –ö–ª–∞—Å—Ç–µ—Ä (YARN/Kubernetes)
```bash
spark-submit \
    --master yarn \
    --deploy-mode cluster \
    --num-executors 4 \
    --executor-memory 4g \
    --executor-cores 2 \
    get_product_category_pairs.py
```

### Databricks/EMR
```python
# –ö–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
# SparkSession —É–∂–µ —Å–æ–∑–¥–∞–Ω–∞ –∫–∞–∫ 'spark'
```

## üêõ –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### –ü—Ä–æ–±–ª–µ–º–∞: "Java not found"
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Java –Ω–∞ Ubuntu/Debian
sudo apt update
sudo apt install openjdk-11-jdk

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Java –Ω–∞ macOS
brew install openjdk@11

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Java –Ω–∞ Windows
# –°–∫–∞—á–∞–π—Ç–µ —Å https://adoptopenjdk.net/
```

### –ü—Ä–æ–±–ª–µ–º–∞: "Insufficient memory"
```python
# –£–≤–µ–ª–∏—á—å—Ç–µ –ø–∞–º—è—Ç—å –¥–ª—è –¥—Ä–∞–π–≤–µ—Ä–∞
spark = SparkSession.builder \
    .config("spark.driver.memory", "4g") \
    .config("spark.driver.maxResultSize", "2g") \
    .getOrCreate()
```

### –ü—Ä–æ–±–ª–µ–º–∞: "Slow performance"
```python
# –í–∫–ª—é—á–∏—Ç–µ –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤
spark = SparkSession.builder \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()
```

## üìà –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö (>1GB)
```python
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ broadcast –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ç–∞–±–ª–∏—Ü
from pyspark.sql.functions import broadcast

result = products_df \
    .join(links_df, "product_id", "left") \
    .join(broadcast(categories_df), "category_id", "left") \
    .select("product_name", "category_name")
```

### –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
```python
# –ï—Å–ª–∏ DataFrame –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ
categories_df.cache()
products_df.cache()

# –ù–µ –∑–∞–±—É–¥—å—Ç–µ –æ—á–∏—Å—Ç–∏—Ç—å –∫—ç—à
categories_df.unpersist()
```

### –ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
```python
# –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö —Ç–∞–±–ª–∏—Ü
products_df = products_df.repartition("product_id")
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ï–¥–∏–Ω–∏—á–Ω—ã–µ —Ç–µ—Å—Ç—ã
```python
def test_products_with_categories():
    """–¢–µ—Å—Ç –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏"""
    result = get_product_category_pairs(products_df, categories_df, links_df)
    paired_products = result.filter(result.category_name.isNotNull())
    assert paired_products.count() > 0

def test_products_without_categories():
    """–¢–µ—Å—Ç –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
    result = get_product_category_pairs(products_df, categories_df, links_df)
    orphaned_products = result.filter(result.category_name.isNull())
    assert orphaned_products.count() >= 0
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
```python
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
result.groupBy("product_name", "category_name").count().filter("count > 1").show()

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ null –∑–Ω–∞—á–µ–Ω–∏—è –≤ product_name
result.filter(result.product_name.isNull()).show()
```

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

### –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- [PySpark SQL Reference](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)

### –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
```python
# –ü—Ä–æ—Å–º–æ—Ç—Ä –ø–ª–∞–Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
result.explain(True)

# –ü—Ä–æ—Å–º–æ—Ç—Ä —Å—Ö–µ–º—ã –¥–∞–Ω–Ω—ã—Ö
result.printSchema()

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º
result.describe().show()
```

## üìÑ –õ–∏—Ü–µ–Ω–∑–∏—è

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Å–æ–∑–¥–∞–Ω –≤ —É—á–µ–±–Ω—ã—Ö —Ü–µ–ª—è—Ö –∏ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.

---

*–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é, —Å–æ–∑–¥–∞–π—Ç–µ Issue –∏–ª–∏ Pull Request.*
